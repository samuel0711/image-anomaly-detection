{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "con_tcc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBRxnpWlUTJ1lTN6SymTIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuel0711/image-anomaly-detection/blob/main/con_tcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSYuFnl5CR4n",
        "outputId": "176df7a8-a61a-4c21-cfb4-70c9b78109dc"
      },
      "source": [
        "import io\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io as skio\n",
        "import torch\n",
        "from torch import nn #neural networks\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "\n",
        "#Carregamento de Dados\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "    sys.path.append('/content/drive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYAT-KAsB42R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8efbfc20-e27c-4b4f-e6be-0e2c3795cba4"
      },
      "source": [
        "args = {\n",
        "    'epoch':50,          #Numero de épocas\n",
        "    'lr':1e-3,            #Taxa de aprendizado\n",
        "    'weight_decay':5e-3,  #Penalidade L2 (Regularização)\n",
        "    'batch_size': 50,     #Tamanho do batch\n",
        "    'num_workers': 4\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  args['device'] = torch.device('cuda')\n",
        "else:\n",
        "  args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])\n",
        "\n",
        "\n",
        "train_set = np.load('train_dataset.npy', allow_pickle = True)\n",
        "test_set = np.load('test_dataset.npy', allow_pickle = True)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ASsbfbGCQjy",
        "outputId": "88df80aa-845d-4433-c4dd-8149de221080"
      },
      "source": [
        "train_set[-1].dtype\n",
        "#train = torch.from_numpy(train_set)\n",
        "#test = torch.from_numpy(test_set)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('int8')"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJiOOTaxmimJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d97209f-73d5-4eef-fc0a-4975287db954"
      },
      "source": [
        "train_loader = DataLoader(train_set, \n",
        "                          batch_size=args['batch_size'], \n",
        "                          shuffle=True, \n",
        "                          num_workers=args['num_workers'])\n",
        "\n",
        "test_loader = DataLoader(test_set, \n",
        "                         batch_size=args['batch_size'], \n",
        "                         shuffle=True, \n",
        "                         num_workers=args['num_workers']) "
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Convolutional Autoencoder\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "       \n",
        "        #Encoder\n",
        "        self.conv1 = nn.Conv2d(1, 4, 3, padding=1)  #in_channel, out_channel, kernel_size\n",
        "        self.conv2 = nn.Conv2d(4, 16, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 4, 3, padding=1) \n",
        "        self.pool = nn.MaxPool2d(2, 2) #kernel_size, stride\n",
        "       \n",
        "        #Decoder\n",
        "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(16, 4, 2, stride=2)\n",
        "        self.t_conv3 = nn.ConvTranspose2d(4, 1, 2, stride=2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        x = F.relu(self.t_conv2(x))\n",
        "        x = torch.sigmoid(self.t_conv3(x))\n",
        "              \n",
        "        return x\n",
        "\n",
        "\n",
        "#Instantiate the model\n",
        "model = ConvAutoencoder()\n"
      ],
      "metadata": {
        "id": "Es1OH4QXblsC"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQhg2NBR46DB"
      },
      "source": [
        "# **Treinando**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-c6HSrSzJ1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086e411e-8fc5-42e4-8a4c-4621c0487540"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "criterio = nn.MSELoss().to(args['device'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "model.to(args['device'])"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvAutoencoder(\n",
              "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (t_conv2): ConvTranspose2d(16, 4, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (t_conv3): ConvTranspose2d(4, 1, kernel_size=(2, 2), stride=(2, 2))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treino ( Somente com imagens não-anomalas)"
      ],
      "metadata": {
        "id": "vMrPLQr4hvrE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GemLuqJp7aIL"
      },
      "source": [
        "def train(train_loader, model, epoch):\n",
        "  ###################\n",
        "  # treinando o modelo #\n",
        "  ###################\n",
        "  model.train()\n",
        "  \n",
        "  start = time.time()\n",
        "\n",
        "  epoch_loss_train = []\n",
        "  train_loss = 0.0\n",
        "  for data in train_loader:\n",
        "    imagem = data.unsqueeze(1).float()\n",
        "    imagem /= 255.0\n",
        "    imagem = imagem.to(args['device'])\n",
        "\n",
        "    # clear the gradients of all optimized variables\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    output = model(imagem)\n",
        "\n",
        "    # calculando loss\n",
        "    loss = criterio(output, imagem)\n",
        "    epoch_loss_train.append(loss.detach().item())\n",
        "\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # próximo passo\n",
        "    optimizer.step()\n",
        "\n",
        "    # update running training loss\n",
        "    train_loss += loss.item()*imagem.size(0)\n",
        "\n",
        "  train_loss = train_loss/len(train_loader)\n",
        "  epoch_loss_train = np.array(epoch_loss_train)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  print('########## Train ##########')\n",
        "  print('Epoch: %d, Loss: %.4f +/- %.4f, Time: %.2f\\n'% (epoch, epoch_loss_train.mean(), epoch_loss_train.std(), end-start))\n",
        "\n",
        "  print(\"Epoca: {}\\t Train Loss: {:.3f}\\n\".format(epoch, train_loss))\n",
        "  #print(\"Medía do Loss:\", epoch_loss_train.mean())\n",
        "\n",
        "  return epoch_loss_train.mean()"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste ( Somente com imagens anômalas )"
      ],
      "metadata": {
        "id": "4SqIE-syh4Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(test_loader, model, epoch):\n",
        "  ###################\n",
        "  # validando o modelo #\n",
        "  ###################\n",
        "  model.eval()\n",
        "\n",
        "  start = time.time()\n",
        "  \n",
        "  epoch_loss_test = []\n",
        "  test_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      imagem = data.unsqueeze(1).float()\n",
        "      imagem /= 255.0\n",
        "      imagem = imagem.to(args['device'])\n",
        "\n",
        "      # forward pass\n",
        "      output = model(imagem)\n",
        "\n",
        "      # calculando loss\n",
        "      loss = criterio(output, imagem)\n",
        "      epoch_loss_test.append(loss.detach().item())\n",
        "\n",
        "      # update running test loss\n",
        "      test_loss += loss.item()*imagem.size(0)\n",
        "\n",
        "    test_loss = test_loss/len(test_loader)\n",
        "    epoch_loss_test = np.array(epoch_loss_test)\n",
        "    end = time.time()\n",
        "\n",
        "    print('########## Test ##########')\n",
        "    print('Epoch: %d, Loss: %.4f +/- %.4f, Time: %.2f\\n'% (epoch, epoch_loss_test.mean(), epoch_loss_test.std(), end-start))\n",
        "    \n",
        "    print(\"Epoca: {}\\t Test Loss: {:.3f}\\n\".format(epoch, test_loss))\n",
        "    #print(\"Medía do Loss:\", epoch_loss_test.mean())\n",
        "\n",
        "    return epoch_loss_test.mean()"
      ],
      "metadata": {
        "id": "991BeROg8xf3"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rodando Teste e Treino"
      ],
      "metadata": {
        "id": "6eTPY7xYolud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "\n",
        "for epoch in range(args['epoch']):\n",
        "  #Train\n",
        "  train_losses.append(train(train_loader, model, epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X16XessjolJa",
        "outputId": "e15611b1-171d-4f2f-dae3-918ea336ddf1"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########## Train ##########\n",
            "Epoch: 0, Loss: 0.2189 +/- 0.1080, Time: 9.99\n",
            "\n",
            "Epoca: 0\t Train Loss: 10.943\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 1, Loss: 0.0726 +/- 0.0011, Time: 9.82\n",
            "\n",
            "Epoca: 1\t Train Loss: 3.632\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 2, Loss: 0.0715 +/- 0.0007, Time: 9.92\n",
            "\n",
            "Epoca: 2\t Train Loss: 3.577\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 3, Loss: 0.0712 +/- 0.0009, Time: 9.89\n",
            "\n",
            "Epoca: 3\t Train Loss: 3.562\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 4, Loss: 0.0711 +/- 0.0008, Time: 9.89\n",
            "\n",
            "Epoca: 4\t Train Loss: 3.554\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 5, Loss: 0.0710 +/- 0.0008, Time: 9.97\n",
            "\n",
            "Epoca: 5\t Train Loss: 3.550\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 6, Loss: 0.0710 +/- 0.0008, Time: 9.99\n",
            "\n",
            "Epoca: 6\t Train Loss: 3.548\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 7, Loss: 0.0709 +/- 0.0009, Time: 9.94\n",
            "\n",
            "Epoca: 7\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 8, Loss: 0.0709 +/- 0.0009, Time: 9.95\n",
            "\n",
            "Epoca: 8\t Train Loss: 3.546\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 9, Loss: 0.0709 +/- 0.0009, Time: 9.97\n",
            "\n",
            "Epoca: 9\t Train Loss: 3.545\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 10, Loss: 0.0709 +/- 0.0008, Time: 9.94\n",
            "\n",
            "Epoca: 10\t Train Loss: 3.545\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 11, Loss: 0.0709 +/- 0.0008, Time: 10.00\n",
            "\n",
            "Epoca: 11\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 12, Loss: 0.0709 +/- 0.0008, Time: 10.03\n",
            "\n",
            "Epoca: 12\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 13, Loss: 0.0709 +/- 0.0008, Time: 9.96\n",
            "\n",
            "Epoca: 13\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 14, Loss: 0.0709 +/- 0.0008, Time: 10.00\n",
            "\n",
            "Epoca: 14\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 15, Loss: 0.0709 +/- 0.0008, Time: 10.01\n",
            "\n",
            "Epoca: 15\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 16, Loss: 0.0709 +/- 0.0008, Time: 10.00\n",
            "\n",
            "Epoca: 16\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 17, Loss: 0.0709 +/- 0.0008, Time: 10.02\n",
            "\n",
            "Epoca: 17\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 18, Loss: 0.0709 +/- 0.0008, Time: 9.99\n",
            "\n",
            "Epoca: 18\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 19, Loss: 0.0709 +/- 0.0009, Time: 9.99\n",
            "\n",
            "Epoca: 19\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 20, Loss: 0.0709 +/- 0.0008, Time: 9.94\n",
            "\n",
            "Epoca: 20\t Train Loss: 3.544\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 21, Loss: 0.0709 +/- 0.0009, Time: 9.98\n",
            "\n",
            "Epoca: 21\t Train Loss: 3.545\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 22, Loss: 0.0709 +/- 0.0008, Time: 9.94\n",
            "\n",
            "Epoca: 22\t Train Loss: 3.545\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 23, Loss: 0.0709 +/- 0.0008, Time: 9.92\n",
            "\n",
            "Epoca: 23\t Train Loss: 3.545\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 24, Loss: 0.0709 +/- 0.0008, Time: 9.94\n",
            "\n",
            "Epoca: 24\t Train Loss: 3.545\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 25, Loss: 0.0709 +/- 0.0009, Time: 9.94\n",
            "\n",
            "Epoca: 25\t Train Loss: 3.545\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 26, Loss: 0.0709 +/- 0.0008, Time: 10.00\n",
            "\n",
            "Epoca: 26\t Train Loss: 3.546\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 27, Loss: 0.0709 +/- 0.0008, Time: 10.03\n",
            "\n",
            "Epoca: 27\t Train Loss: 3.546\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 28, Loss: 0.0709 +/- 0.0008, Time: 9.98\n",
            "\n",
            "Epoca: 28\t Train Loss: 3.546\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 29, Loss: 0.0709 +/- 0.0009, Time: 9.96\n",
            "\n",
            "Epoca: 29\t Train Loss: 3.546\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 30, Loss: 0.0709 +/- 0.0008, Time: 9.99\n",
            "\n",
            "Epoca: 30\t Train Loss: 3.546\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 31, Loss: 0.0709 +/- 0.0008, Time: 9.97\n",
            "\n",
            "Epoca: 31\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 32, Loss: 0.0709 +/- 0.0008, Time: 9.98\n",
            "\n",
            "Epoca: 32\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 33, Loss: 0.0709 +/- 0.0008, Time: 9.97\n",
            "\n",
            "Epoca: 33\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 34, Loss: 0.0709 +/- 0.0008, Time: 9.99\n",
            "\n",
            "Epoca: 34\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 35, Loss: 0.0709 +/- 0.0008, Time: 10.03\n",
            "\n",
            "Epoca: 35\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 36, Loss: 0.0709 +/- 0.0008, Time: 10.04\n",
            "\n",
            "Epoca: 36\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 37, Loss: 0.0709 +/- 0.0008, Time: 9.93\n",
            "\n",
            "Epoca: 37\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 38, Loss: 0.0709 +/- 0.0008, Time: 9.85\n",
            "\n",
            "Epoca: 38\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 39, Loss: 0.0709 +/- 0.0008, Time: 9.88\n",
            "\n",
            "Epoca: 39\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 40, Loss: 0.0709 +/- 0.0008, Time: 9.84\n",
            "\n",
            "Epoca: 40\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 41, Loss: 0.0709 +/- 0.0009, Time: 9.87\n",
            "\n",
            "Epoca: 41\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 42, Loss: 0.0709 +/- 0.0008, Time: 9.89\n",
            "\n",
            "Epoca: 42\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 43, Loss: 0.0709 +/- 0.0009, Time: 9.97\n",
            "\n",
            "Epoca: 43\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 44, Loss: 0.0709 +/- 0.0008, Time: 9.95\n",
            "\n",
            "Epoca: 44\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 45, Loss: 0.0709 +/- 0.0008, Time: 9.96\n",
            "\n",
            "Epoca: 45\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 46, Loss: 0.0709 +/- 0.0008, Time: 9.96\n",
            "\n",
            "Epoca: 46\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 47, Loss: 0.0709 +/- 0.0008, Time: 9.96\n",
            "\n",
            "Epoca: 47\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 48, Loss: 0.0709 +/- 0.0009, Time: 9.95\n",
            "\n",
            "Epoca: 48\t Train Loss: 3.547\n",
            "\n",
            "########## Train ##########\n",
            "Epoch: 49, Loss: 0.0709 +/- 0.0008, Time: 9.95\n",
            "\n",
            "Epoca: 49\t Train Loss: 3.547\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "\n",
        "for epoch in range(args['epoch']):\n",
        "\n",
        "  #Validate\n",
        "  test_losses.append(validate(test_loader, model, epoch))"
      ],
      "metadata": {
        "id": "uyvSJv9t5hCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Treino x Teste"
      ],
      "metadata": {
        "id": "0rXPlSNFijQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " plt.plot(train_losses)\n",
        " plt.plot(test_losses)\n",
        " plt.ylabel('Loss')\n",
        " plt.xlabel('Epochs')"
      ],
      "metadata": {
        "id": "nUMrkYR5iiqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot da Imagem Original(normalizada) x Imagem Reconstruída via AutoEncoder Convolucional"
      ],
      "metadata": {
        "id": "AHoIUvBmirI9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogQCkTYUE9cY"
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images = dataiter.next()\n",
        "\n",
        "imagem = images.unsqueeze(1).float()\n",
        "imagem /= 255.0\n",
        "imagem = imagem.to(args['device'])\n",
        "\n",
        "# get sample outputs\n",
        "output = model(imagem)\n",
        "# prep images for display\n",
        "images = images.numpy()\n",
        "\n",
        "\n",
        "# output is resized into a batch of iages\n",
        "output = output.view(args['batch_size'], 1, 160, 240)\n",
        "# use detach when it's an output that requires_grad\n",
        "output = output.detach()#.numpy()\n",
        "\n",
        "\n",
        "\n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(output[idx].squeeze().cpu(), cmap='gray')\n",
        "    \n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(images[idx].squeeze(), cmap='gray')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7OFeSiAFh_1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
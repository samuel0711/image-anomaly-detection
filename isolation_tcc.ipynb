{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuel0711/image-anomaly-detection/blob/main/isolation_tcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSYuFnl5CR4n",
        "outputId": "cdd57d5f-38aa-4bc4-c134-aeb441cfed67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import sys\n",
        "import os\n",
        "import gc\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io as skio\n",
        "import torch\n",
        "from torch import nn #neural networks\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "#Carregamento de Dados\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "    sys.path.append('/content/drive/My Drive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYAT-KAsB42R",
        "outputId": "f48e82ae-d189-41e2-ca10-4c2d9e03e6b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "args = {\n",
        "    'epoch':100,          #Numero de épocas\n",
        "    'lr':5e-4,            #Taxa de aprendizado\n",
        "    'weight_decay':5e-5,  #Penalidade L2 (Regularização)\n",
        "    'batch_size': 50,     #Tamanho do batch\n",
        "    'num_workers': 2\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  args['device'] = torch.device('cuda')\n",
        "else:\n",
        "  args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])\n",
        "\n",
        "\n",
        "train_set = np.load('train_dataset.npy', allow_pickle = True)/255.0\n",
        "test_set = np.load('test_dataset.npy', allow_pickle = True)/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj4IxvZ5BC-8",
        "outputId": "93c4092b-7fe2-4563-c612-3f90765cbd37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:212: RuntimeWarning: overflow encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treino\n",
            "Média:  0.3945 \t Desvio Padrão:  inf \n",
            "\n",
            "\n",
            "Teste\n",
            "Média:  0.44 \t Desvio Padrão:  inf\n"
          ]
        }
      ],
      "source": [
        "print('Treino\\nMédia: ',train_set.mean(),'\\t Desvio Padrão: ',train_set.std(),'\\n\\n')\n",
        "print('Teste\\nMédia: ',test_set.mean(),'\\t Desvio Padrão: ',test_set.std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iXr6tMUp0c_U"
      },
      "outputs": [],
      "source": [
        "train_set = torch.from_numpy(train_set)\n",
        "test_set = torch.from_numpy(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rJiOOTaxmimJ"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, \n",
        "                          batch_size=args['batch_size'], \n",
        "                          shuffle=True, \n",
        "                          num_workers=args['num_workers'])\n",
        "\n",
        "test_loader = DataLoader(test_set, \n",
        "                         batch_size=args['batch_size'], \n",
        "                         shuffle=True, \n",
        "                         num_workers=args['num_workers']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Es1OH4QXblsC"
      },
      "outputs": [],
      "source": [
        "#Define the Convolutional Autoencoder\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "       \n",
        "        #Encoder\n",
        "        self.conv1 = nn.Conv2d(1, 64, 3, padding = 1)  #in_channel, out_channel, kernel_size\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding = 1)\n",
        "        self.conv3 = nn.Conv2d(128, 128, 3, padding = 1) \n",
        "        self.conv4 = nn.Conv2d(128, 254, 3, padding = 1) \n",
        "        self.pool = nn.MaxPool2d(2) #kernel_size, stride\n",
        "       \n",
        "        #Decoder\n",
        "        self.t_conv1 = nn.ConvTranspose2d(254, 128, 2, stride=2)        \n",
        "        self.t_conv2 = nn.ConvTranspose2d(128, 128, 2, stride=2)\n",
        "        self.t_conv3 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.t_conv4 = nn.ConvTranspose2d(64, 1, 2, stride=2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.leaky_relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        x = F.relu(self.t_conv2(x))\n",
        "        x = F.relu(self.t_conv3(x))\n",
        "        x = torch.sigmoid(self.t_conv4(x))\n",
        "              \n",
        "        return x\n",
        "\n",
        "\n",
        "#Instantiate the model\n",
        "model = ConvAutoencoder()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQhg2NBR46DB"
      },
      "source": [
        "# **Treinando**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-c6HSrSzJ1V",
        "outputId": "c24bfdc8-34f1-4824-8291-a244f31c1605"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvAutoencoder(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(128, 254, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (t_conv1): ConvTranspose2d(254, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (t_conv2): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (t_conv3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (t_conv4): ConvTranspose2d(64, 1, kernel_size=(2, 2), stride=(2, 2))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from torch import optim\n",
        "\n",
        "criterio = nn.MSELoss().to(args['device'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "model.to(args['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_s9WKltZrPdU"
      },
      "outputs": [],
      "source": [
        "def save_decod_img(img, output, epoch, name):\n",
        "    img = img.view(img.size(0), 1, img.shape[2], img.shape[3])\n",
        "    output = output.view(output.size(0), 1, output.shape[2], output.shape[3])\n",
        "    error = output.view(output.size(0), 1, output.shape[2], output.shape[3]) - img.view(img.size(0), 1, img.shape[2], img.shape[3])\n",
        "    save_image(img, './imgs_tcc/{}_imagem_image{}.png'.format(name,epoch))\n",
        "    save_image(output, './imgs_tcc/{}_output_image{}.png'.format(name,epoch))\n",
        "    save_image(error, './imgs_tcc/{}_error_image{}.png'.format(name,epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMrPLQr4hvrE"
      },
      "source": [
        "## Treino ( Somente com imagens não-anomalas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GemLuqJp7aIL"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, epoch, name):\n",
        "  ###################\n",
        "  # treinando o modelo #\n",
        "  ###################\n",
        "  model.train()\n",
        "  \n",
        "  start = time.time()\n",
        "\n",
        "  tns = []\n",
        "\n",
        "  train_loss = 0.0\n",
        "  for data in train_loader:\n",
        "    imagem = data.unsqueeze(1).float()\n",
        "    imagem = imagem.to(args['device'])\n",
        "\n",
        "    #imagem = imagem/255.0\n",
        "\n",
        "    # clear the gradients of all optimized variables\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    output = model(imagem)\n",
        "\n",
        "    # calculando loss\n",
        "    loss = criterio(output, imagem)\n",
        "    #isolation_forest.fit(torch.flatten(output,start_dim=1).cpu().data)\n",
        "\n",
        "\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # próximo passo\n",
        "    optimizer.step()\n",
        "\n",
        "    # update running training loss\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if epoch == args['epoch']:\n",
        "      for x, y in zip(imagem, output):\n",
        "          tns.append(np.array(y.squeeze().cpu().data - x.squeeze().cpu().data))\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    save_decod_img(imagem.cpu().data, output.cpu().data, epoch, name)\n",
        "\n",
        "\n",
        "  train_loss = train_loss/len(train_loader)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "\n",
        "  print('########## Train ##########')\n",
        "\n",
        "  print(\"Epoca: {}\\t Train Loss: {:.6f}, Time: {:.2f}\\n\".format(epoch, train_loss, end-start))\n",
        "  #print(\"Medía do Loss:\", epoch_loss_train.mean())\n",
        "\n",
        "  return train_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SqIE-syh4Dm"
      },
      "source": [
        "## Validação ( Somente com imagens anômalas )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "991BeROg8xf3"
      },
      "outputs": [],
      "source": [
        "def validate(test_loader, model, epoch, name):\n",
        "  ###################\n",
        "  # validando o modelo #\n",
        "  ###################\n",
        "  model.eval()\n",
        "\n",
        "  tns = []\n",
        "  predicts = []\n",
        "  outliers, inliers = 0, 0\n",
        "  \n",
        "  start = time.time()\n",
        "  \n",
        "  test_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      imagem = data.unsqueeze(1).float()\n",
        "      imagem = imagem.to(args['device'])\n",
        "\n",
        "      #imagem = imagem/255.0\n",
        "      # forward pass\n",
        "      output = model(imagem)\n",
        "\n",
        "      #y = isolation_forest.predict(torch.flatten(output,start_dim=1).cpu().data)\n",
        "      #outliers += list(y).count(-1)\n",
        "      #inliers += list(y).count(1)\n",
        "      #predicts.append(list(y).count(-1)/y.shape[0])\n",
        "\n",
        "      # calculando loss\n",
        "      loss = criterio(output, imagem)\n",
        "\n",
        "      # update running test loss\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      if epoch == (args['epoch']-1):\n",
        "        for x, y in zip(imagem, output):\n",
        "          tns.append(y.squeeze().cpu().numpy() - x.squeeze().cpu().numpy())\n",
        "\n",
        "    test_loss = test_loss/len(test_loader)\n",
        "    end = time.time()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      save_decod_img(imagem.cpu().data, output.cpu().data, epoch, name)\n",
        "    print('########## Validate ##########')\n",
        "\n",
        "    \n",
        "    print(\"Epoca: {}\\t Val Loss: {:.6f}, Time: {:.2f}\\n\".format(epoch, test_loss, end-start)) #\\nAcurácia do Isolation Forest: {:.3f}\\t {} Outliers, {} Inliers\\n\".format(epoch, test_loss, end-start, (sum(predicts)/len(predicts)), outliers, inliers))\n",
        "\n",
        "    return test_loss#, (sum(predicts)/len(predicts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eTPY7xYolud"
      },
      "source": [
        "## Rodando Validação e Treino\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X16XessjolJa",
        "outputId": "eb4492e3-6fb0-4253-a883-9a582f3b6701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########## Train ##########\n",
            "Epoca: 0\t Train Loss: 0.012458, Time: 20.58\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 0\t Val Loss: 0.003931, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 1\t Train Loss: 0.003347, Time: 17.83\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 1\t Val Loss: 0.003025, Time: 1.91\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 2\t Train Loss: 0.002424, Time: 17.96\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 2\t Val Loss: 0.002513, Time: 1.92\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 3\t Train Loss: 0.001959, Time: 18.09\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 3\t Val Loss: 0.002266, Time: 1.92\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 4\t Train Loss: 0.001657, Time: 18.24\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 4\t Val Loss: 0.002040, Time: 1.92\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 5\t Train Loss: 0.001463, Time: 21.22\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 5\t Val Loss: 0.001930, Time: 1.98\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 6\t Train Loss: 0.001332, Time: 18.52\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 6\t Val Loss: 0.001856, Time: 1.93\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 7\t Train Loss: 0.001244, Time: 18.56\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 7\t Val Loss: 0.001753, Time: 1.93\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 8\t Train Loss: 0.001175, Time: 18.67\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 8\t Val Loss: 0.001707, Time: 1.94\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 9\t Train Loss: 0.001121, Time: 18.78\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 9\t Val Loss: 0.001653, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 10\t Train Loss: 0.001076, Time: 21.69\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 10\t Val Loss: 0.001629, Time: 1.99\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 11\t Train Loss: 0.001037, Time: 18.88\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 11\t Val Loss: 0.001597, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 12\t Train Loss: 0.001006, Time: 18.91\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 12\t Val Loss: 0.001555, Time: 1.94\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 13\t Train Loss: 0.000978, Time: 18.97\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 13\t Val Loss: 0.001546, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 14\t Train Loss: 0.000950, Time: 19.03\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 14\t Val Loss: 0.001508, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 15\t Train Loss: 0.000946, Time: 21.42\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 15\t Val Loss: 0.001479, Time: 1.99\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 16\t Train Loss: 0.000900, Time: 19.04\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 16\t Val Loss: 0.001496, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 17\t Train Loss: 0.000893, Time: 19.09\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 17\t Val Loss: 0.001452, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 18\t Train Loss: 0.000869, Time: 19.04\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 18\t Val Loss: 0.001437, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 19\t Train Loss: 0.000856, Time: 19.02\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 19\t Val Loss: 0.001420, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 20\t Train Loss: 0.000843, Time: 21.81\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 20\t Val Loss: 0.001399, Time: 2.01\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 21\t Train Loss: 0.000828, Time: 19.08\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 21\t Val Loss: 0.001439, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 22\t Train Loss: 0.000812, Time: 19.17\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 22\t Val Loss: 0.001398, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 23\t Train Loss: 0.000805, Time: 19.03\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 23\t Val Loss: 0.001405, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 24\t Train Loss: 0.000789, Time: 18.98\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 24\t Val Loss: 0.001367, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 25\t Train Loss: 0.000780, Time: 21.92\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 25\t Val Loss: 0.001339, Time: 1.99\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 26\t Train Loss: 0.000771, Time: 19.10\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 26\t Val Loss: 0.001330, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 27\t Train Loss: 0.000760, Time: 19.11\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 27\t Val Loss: 0.001319, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 28\t Train Loss: 0.000747, Time: 19.05\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 28\t Val Loss: 0.001342, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 29\t Train Loss: 0.000743, Time: 18.98\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 29\t Val Loss: 0.001314, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 30\t Train Loss: 0.000731, Time: 21.68\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 30\t Val Loss: 0.001297, Time: 2.01\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 31\t Train Loss: 0.000723, Time: 19.09\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 31\t Val Loss: 0.001294, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 32\t Train Loss: 0.000717, Time: 19.16\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 32\t Val Loss: 0.001291, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 33\t Train Loss: 0.000704, Time: 19.04\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 33\t Val Loss: 0.001293, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 34\t Train Loss: 0.000699, Time: 18.97\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 34\t Val Loss: 0.001265, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 35\t Train Loss: 0.000696, Time: 21.69\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 35\t Val Loss: 0.001256, Time: 2.00\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 36\t Train Loss: 0.000686, Time: 19.10\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 36\t Val Loss: 0.001253, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 37\t Train Loss: 0.000679, Time: 19.12\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 37\t Val Loss: 0.001238, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 38\t Train Loss: 0.000676, Time: 19.03\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 38\t Val Loss: 0.001244, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 39\t Train Loss: 0.000663, Time: 19.00\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 39\t Val Loss: 0.001244, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 40\t Train Loss: 0.000660, Time: 21.29\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 40\t Val Loss: 0.001222, Time: 1.99\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 41\t Train Loss: 0.000658, Time: 19.05\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 41\t Val Loss: 0.001222, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 42\t Train Loss: 0.000645, Time: 19.18\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 42\t Val Loss: 0.001229, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 43\t Train Loss: 0.000644, Time: 19.04\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 43\t Val Loss: 0.001218, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 44\t Train Loss: 0.000638, Time: 18.99\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 44\t Val Loss: 0.001215, Time: 1.94\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 45\t Train Loss: 0.000635, Time: 21.40\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 45\t Val Loss: 0.001199, Time: 1.99\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 46\t Train Loss: 0.000629, Time: 19.11\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 46\t Val Loss: 0.001189, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 47\t Train Loss: 0.000619, Time: 19.12\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 47\t Val Loss: 0.001181, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 48\t Train Loss: 0.000617, Time: 19.02\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 48\t Val Loss: 0.001178, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 49\t Train Loss: 0.000612, Time: 19.01\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 49\t Val Loss: 0.001209, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 50\t Train Loss: 0.000611, Time: 21.70\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 50\t Val Loss: 0.001175, Time: 2.03\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 51\t Train Loss: 0.000603, Time: 19.11\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 51\t Val Loss: 0.001162, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 52\t Train Loss: 0.000606, Time: 19.18\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 52\t Val Loss: 0.001170, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 53\t Train Loss: 0.000591, Time: 19.03\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 53\t Val Loss: 0.001165, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 54\t Train Loss: 0.000591, Time: 19.00\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 54\t Val Loss: 0.001201, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 55\t Train Loss: 0.000588, Time: 21.59\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 55\t Val Loss: 0.001169, Time: 2.01\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 56\t Train Loss: 0.000583, Time: 19.12\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 56\t Val Loss: 0.001157, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 57\t Train Loss: 0.000579, Time: 19.18\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 57\t Val Loss: 0.001134, Time: 1.94\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 58\t Train Loss: 0.000577, Time: 19.02\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 58\t Val Loss: 0.001161, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 59\t Train Loss: 0.000571, Time: 19.01\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 59\t Val Loss: 0.001145, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 60\t Train Loss: 0.000570, Time: 21.24\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 60\t Val Loss: 0.001129, Time: 2.01\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 61\t Train Loss: 0.000565, Time: 19.12\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 61\t Val Loss: 0.001119, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 62\t Train Loss: 0.000559, Time: 19.18\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 62\t Val Loss: 0.001129, Time: 1.94\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 63\t Train Loss: 0.000559, Time: 19.05\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 63\t Val Loss: 0.001114, Time: 1.94\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 64\t Train Loss: 0.000555, Time: 19.04\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 64\t Val Loss: 0.001123, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 65\t Train Loss: 0.000549, Time: 21.82\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 65\t Val Loss: 0.001144, Time: 2.02\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 66\t Train Loss: 0.000547, Time: 19.13\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 66\t Val Loss: 0.001103, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 67\t Train Loss: 0.000546, Time: 19.16\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 67\t Val Loss: 0.001097, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 68\t Train Loss: 0.000542, Time: 19.05\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 68\t Val Loss: 0.001111, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 69\t Train Loss: 0.000535, Time: 18.98\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 69\t Val Loss: 0.001116, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 70\t Train Loss: 0.000536, Time: 21.60\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 70\t Val Loss: 0.001092, Time: 1.99\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 71\t Train Loss: 0.000533, Time: 19.15\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 71\t Val Loss: 0.001098, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 72\t Train Loss: 0.000528, Time: 19.13\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 72\t Val Loss: 0.001082, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 73\t Train Loss: 0.000533, Time: 19.05\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 73\t Val Loss: 0.001102, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 74\t Train Loss: 0.000516, Time: 19.04\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 74\t Val Loss: 0.001100, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 75\t Train Loss: 0.000522, Time: 21.49\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 75\t Val Loss: 0.001098, Time: 2.01\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 76\t Train Loss: 0.000518, Time: 19.09\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 76\t Val Loss: 0.001155, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 77\t Train Loss: 0.000516, Time: 19.18\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 77\t Val Loss: 0.001082, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 78\t Train Loss: 0.000515, Time: 19.06\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 78\t Val Loss: 0.001076, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 79\t Train Loss: 0.000508, Time: 19.02\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 79\t Val Loss: 0.001065, Time: 1.95\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 80\t Train Loss: 0.000507, Time: 21.81\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 80\t Val Loss: 0.001063, Time: 2.01\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 81\t Train Loss: 0.000505, Time: 19.15\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 81\t Val Loss: 0.001067, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 82\t Train Loss: 0.000503, Time: 19.14\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 82\t Val Loss: 0.001057, Time: 1.96\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 83\t Train Loss: 0.000500, Time: 19.08\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 83\t Val Loss: 0.001080, Time: 1.98\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 84\t Train Loss: 0.000497, Time: 19.08\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 84\t Val Loss: 0.001052, Time: 1.98\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 85\t Train Loss: 0.000496, Time: 21.60\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 85\t Val Loss: 0.001061, Time: 2.01\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 86\t Train Loss: 0.000489, Time: 19.18\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 86\t Val Loss: 0.001054, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 87\t Train Loss: 0.000492, Time: 19.23\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 87\t Val Loss: 0.001051, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 88\t Train Loss: 0.000487, Time: 19.10\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 88\t Val Loss: 0.001047, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 89\t Train Loss: 0.000485, Time: 19.07\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 89\t Val Loss: 0.001050, Time: 1.98\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 90\t Train Loss: 0.000486, Time: 21.78\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 90\t Val Loss: 0.001047, Time: 2.02\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 91\t Train Loss: 0.000478, Time: 19.19\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 91\t Val Loss: 0.001043, Time: 1.98\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 92\t Train Loss: 0.000481, Time: 19.21\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 92\t Val Loss: 0.001034, Time: 1.98\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 93\t Train Loss: 0.000475, Time: 19.10\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 93\t Val Loss: 0.001031, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 94\t Train Loss: 0.000474, Time: 19.10\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 94\t Val Loss: 0.001035, Time: 1.98\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 95\t Train Loss: 0.000474, Time: 21.57\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 95\t Val Loss: 0.001040, Time: 2.00\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 96\t Train Loss: 0.000469, Time: 19.17\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 96\t Val Loss: 0.001032, Time: 1.98\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 97\t Train Loss: 0.000469, Time: 19.24\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 97\t Val Loss: 0.001031, Time: 1.97\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 98\t Train Loss: 0.000463, Time: 19.08\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 98\t Val Loss: 0.001036, Time: 1.98\n",
            "\n",
            "########## Train ##########\n",
            "Epoca: 99\t Train Loss: 0.000465, Time: 19.11\n",
            "\n",
            "########## Validate ##########\n",
            "Epoca: 99\t Val Loss: 0.001036, Time: 2.34\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_losses, val_losses = [], []\n",
        "val_predicts = []\n",
        "\n",
        "i = 1\n",
        "for epoch in range(args['epoch']):\n",
        "  #Train\n",
        "  train_loss = train(train_loader, model, epoch, 'train')\n",
        "  train_losses.append(train_loss)\n",
        "\n",
        "  val_loss = validate(test_loader, model, epoch, 'test')\n",
        "  val_losses.append(val_loss)\n",
        "  #val_predicts.append(predicts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSLDlucDFZ5M"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = None\n",
        "test_set = None"
      ],
      "metadata": {
        "id": "xitvNSirDKDN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''isolation_forest = IsolationForest()\n",
        "\n",
        "isolation_forest.fit( np.reshape(train_set, (train_set.shape[0],train_set.shape[1]*train_set.shape[2])) )\n",
        "\n",
        "inlier_pred_test = isolation_forest.predict(np.reshape(train_set, (train_set.shape[0],train_set.shape[1]*train_set.shape[2])))\n",
        "outlier_pred = isolation_forest.predict(np.reshape(test_set, (test_set.shape[0],test_set.shape[1]*test_set.shape[2])))\n",
        "\n",
        "print(\"Accuracy in Detecting Legit Cases:\", list(inlier_pred_test).count(1)/inlier_pred_test.shape[0])\n",
        "print(\"Accuracy in Detecting Fraud Cases:\", list(outlier_pred).count(-1)/outlier_pred.shape[0])'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "UlaNLGUv-mJi",
        "outputId": "6b33d045-e1ad-4e94-d2ee-4d9c0c1ee36f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'isolation_forest = IsolationForest()\\n\\nisolation_forest.fit( np.reshape(train_set, (train_set.shape[0],train_set.shape[1]*train_set.shape[2])) )\\n\\ninlier_pred_test = isolation_forest.predict(np.reshape(train_set, (train_set.shape[0],train_set.shape[1]*train_set.shape[2])))\\noutlier_pred = isolation_forest.predict(np.reshape(test_set, (test_set.shape[0],test_set.shape[1]*test_set.shape[2])))\\n\\nprint(\"Accuracy in Detecting Legit Cases:\", list(inlier_pred_test).count(1)/inlier_pred_test.shape[0])\\nprint(\"Accuracy in Detecting Fraud Cases:\", list(outlier_pred).count(-1)/outlier_pred.shape[0])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "tns = []\n",
        "\n",
        "for imagem in train_loader:\n",
        "  imagem = imagem.unsqueeze(1).float()\n",
        "  imagem = imagem.to(args['device'])\n",
        "\n",
        "  # forward pass\n",
        "  output = model(imagem)\n",
        "\n",
        "  # calculando loss\n",
        "  loss = criterio(output, imagem)\n",
        "\n",
        "  for img in output:\n",
        "    tns.append(img.squeeze().cpu().data.numpy())\n",
        "\n",
        "\n",
        "print(\"Loss: \",loss.item())"
      ],
      "metadata": {
        "id": "69aXv3N6FpBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78104db1-4587-4b48-e7eb-81aa6291de77"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.00044387156958691776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "isolation_forest = IsolationForest()\n",
        "\n",
        "inlier_dataset = np.array(tns)\n",
        "\n",
        "inlier_dataset = np.reshape(inlier_dataset, (inlier_dataset.shape[0],inlier_dataset.shape[1]*inlier_dataset.shape[2]))\n",
        "\n",
        "isolation_forest.fit(inlier_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqozyBhV9p68",
        "outputId": "610d0b5c-9b91-4ef1-f242-f2a742979a88"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IsolationForest()"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3CAobTgLFe3e"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "tns = []\n",
        "\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "  for imagem in test_loader:\n",
        "    imagem = imagem.unsqueeze(1).float()\n",
        "    imagem = imagem.to(args['device'])\n",
        "\n",
        "    # forward pass\n",
        "    output = model(imagem)\n",
        "\n",
        "    for img in output:\n",
        "      tns.append(img.squeeze().cpu().data.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inlier_dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Lg65-ZrvrR",
        "outputId": "7a84e3cb-78ea-4a42-9f75-099f841aaf55"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 25600)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outlier_dataset = np.array(tns)\n",
        "\n",
        "inlier_pred_test = isolation_forest.predict(inlier_dataset)\n",
        "outlier_pred = isolation_forest.predict(np.reshape(outlier_dataset, (outlier_dataset.shape[0],outlier_dataset.shape[1]*outlier_dataset.shape[2])))\n",
        "\n",
        "print(\"Accuracy in Detecting Legit Cases:\", list(inlier_pred_test).count(1)/inlier_pred_test.shape[0])\n",
        "print(\"Accuracy in Detecting Fraud Cases:\", list(outlier_pred).count(-1)/outlier_pred.shape[0])"
      ],
      "metadata": {
        "id": "Asro62YLD0QB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8995e1a9-1e9f-4fca-cd35-4249d78ae277"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy in Detecting Legit Cases: 0.925625\n",
            "Accuracy in Detecting Fraud Cases: 0.716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outlier_dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7rDa3qQPiH4",
        "outputId": "2ef6ed5e-3a2b-4813-dc3e-e0e6662672cd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 160, 160)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.concatenate((inlier_dataset,np.reshape(outlier_dataset, (outlier_dataset.shape[0],outlier_dataset.shape[1]*outlier_dataset.shape[2]))))\n",
        "\n",
        "label = np.concatenate((inlier_pred_test,outlier_pred))\n",
        "\n",
        "dataset_pred = isolation_forest.predict(dataset)\n",
        "\n",
        "print(\"Accuracy of all dataset: \",accuracy_score(label,dataset_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ulwEJgc0pVr",
        "outputId": "2be70ed2-cd04-4a78-82b3-53af672579d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of all dataset:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rXPlSNFijQU"
      },
      "source": [
        "## Plot Treino x Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUMrkYR5iiqC"
      },
      "outputs": [],
      "source": [
        " plt.plot(train_losses, label='Train')\n",
        " plt.plot(val_losses, label='Test')\n",
        " plt.legend(loc=\"upper right\")\n",
        " plt.ylabel('Loss')\n",
        " plt.xlabel('Epochs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHoIUvBmirI9"
      },
      "source": [
        "## Plot da Imagem Original(normalizada) x Imagem Reconstruída via AutoEncoder Convolucional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNOb9zHIiohP"
      },
      "outputs": [],
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(train_loader)\n",
        "images = dataiter.next()\n",
        "\n",
        "imagem = images.unsqueeze(1).float()\n",
        "imagem /= 255.0\n",
        "imagem = imagem.to(args['device'])\n",
        "\n",
        "# get sample outputs\n",
        "output = model(imagem)\n",
        "# prep images for display\n",
        "images = images.numpy()\n",
        "\n",
        "\n",
        "\n",
        "# output is resized into a batch of iages\n",
        "output = output.view(args['batch_size'], 1, 160, 160)\n",
        "# use detach when it's an output that requires_grad\n",
        "output = output.detach()#.numpy()\n",
        "\n",
        "\n",
        "\n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    loss = criterio(output[idx], imagem[idx])\n",
        "    label = 'Loss value: {:.3f}'\n",
        "    ax.set_xlabel(label.format(loss) )\n",
        "    ax.set_title(\"Reconstructed Image\")\n",
        "    plt.imshow(output[idx].squeeze().cpu(), cmap='gray')\n",
        "    \n",
        "# plot the first ten input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    loss = criterio(output[idx], imagem[idx])\n",
        "    label = 'Loss value: {:.3f}'\n",
        "    ax.set_xlabel(label.format(loss) )\n",
        "    ax.set_title(\"Original Image\")\n",
        "    plt.imshow(images[idx].squeeze(), cmap='gray')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "isolation_tcc.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDmo1pOwU42tS9TI2v9img",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}